{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# imports\n",
    "from csv import DictReader, DictWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "og_column_names = ['station_id', 'longitude', 'latitude', 'time', 'AtmospherePressure', 'WindDirection', 'WindSpeed', 'Gust', 'WaveHeight', 'WavePeriod', 'MeanWaveDirection', 'Hmax', 'AirTemperature', 'DewPoint', 'SeaTemperature', 'RelativeHumidity']\n",
    "\n",
    "column_names = ['AtmospherePressure', 'WindDirection', 'WindSpeed', 'Gust', 'AirTemperature', 'SeaTemperature', 'RelativeHumidity', 'WaveHeight', 'WavePeriod']\n",
    "\n",
    "with open('../data/raw_data.csv', 'r') as f1, open('../data/clean_data.csv', 'w') as f2:\n",
    "    reader = DictReader(f1, fieldnames=og_column_names)\n",
    "    writer = DictWriter(f2, fieldnames=column_names, lineterminator='\\n')\n",
    "\n",
    "    next(reader)\n",
    "    next(reader)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for line in tqdm(reader):\n",
    "        if any(line[column_name] == 'NaN' for column_name in column_names):\n",
    "            continue\n",
    "        writer.writerow({column_name: line[column_name] for column_name in column_names})\n"
   ],
   "id": "a8ce018daf71eabe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('../data/clean_data.csv')\n",
    "df.describe()"
   ],
   "id": "59a4f1e9a5f5a79f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use the interquartile range to remove the outliers in data.\n",
    "for column_name in column_names:\n",
    "\n",
    "    Q1 = df[column_name].quantile(0.25)\n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_whisker = Q1 - 1.5*IQR\n",
    "    upper_whisker = Q3 + 1.5*IQR\n",
    "    df = df[(df[column_name] >= lower_whisker) & (df[column_name] <= upper_whisker)]\n",
    "\n",
    "df.describe()"
   ],
   "id": "7218ed5003cb4bcb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "features = column_names[:-2]\n",
    "labels = column_names[-2:]\n",
    "\n",
    "X = df[features].to_numpy()\n",
    "# Note that we have 2 labels\n",
    "Y = df[labels].to_numpy()"
   ],
   "id": "643f20578b855660",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The commonly used split of 75% for training, 10% for validation, and 15%\n",
    "# for the test sets has been followed when splitting the dataset.\n",
    "# V Roshan Joseph. Optimal ratio for data splitting. Statistical Analysis and Data Mining: The\n",
    "# ASA Data Science Journal, 15(4):531â€“538, 2022.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_test, Y_test, test_size=0.6, random_state=42)\n",
    "\n",
    "# y1: WaveHeight\n",
    "y1_train = Y_train[:,0]\n",
    "y1_val = Y_val[:,0]\n",
    "y1_test = Y_test[:,0]\n",
    "\n",
    "# y2: WavePeriod\n",
    "y2_train = Y_train[:,1]\n",
    "y2_val = Y_val[:,1]\n",
    "y2_test = Y_test[:,1]"
   ],
   "id": "39ced278bd4212fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def finding_polynomial(X_train, X_val, y_train, y_val, highest_degree=3):\n",
    "    # define a list of values for polynomial degrees\n",
    "    degrees = [i for i in range(1, highest_degree + 1)]\n",
    "    # declare a variable to store the resulting validation errors for each polynomial degree\n",
    "    val_errors = []\n",
    "    for i in range(len(degrees)):    # use for-loop to fit polynomial regression models with different degrees\n",
    "\n",
    "        poly = PolynomialFeatures(degree=degrees[i])\n",
    "        X_train_poly = poly.fit_transform(X_train)\n",
    "        X_val_poly = poly.fit_transform(X_val)\n",
    "\n",
    "        regression = LinearRegression()\n",
    "        regression.fit(X_train_poly, y_train)\n",
    "\n",
    "        y_pred = regression.predict(X_val_poly)\n",
    "\n",
    "        val_error = mean_squared_error(y_val, y_pred)\n",
    "\n",
    "        val_errors.append(val_error)\n",
    "\n",
    "    return val_errors"
   ],
   "id": "bc84a333e56be8fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "val_errors_1 = finding_polynomial(X_train, X_val, y1_train, y1_val)\n",
    "val_errors_2 = finding_polynomial(X_train, X_val, y2_train, y2_val)\n",
    "print(val_errors_1)\n",
    "print(val_errors_2)"
   ],
   "id": "81a220f6d4589b13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6d44970d62551e0d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
